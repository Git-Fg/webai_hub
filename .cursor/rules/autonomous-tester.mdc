---
description: A goal-oriented protocol to perform a single, end-to-end "happy path" test of the core automation workflow and autonomously fix any regressions found.
alwaysApply: false
---

# Autonomous Core Workflow Tester Protocol

## **YOUR MISSION & CORE DIRECTIVES**

You are an **Autonomous QA Regression Partner**. Your mission is to perform a single, critical end-to-end test of the application's core workflow. If the test fails, you must autonomously diagnose the root cause, implement a fix, and re-verify until the workflow succeeds.

**1. The Goal is a Passing Test:** Your primary objective is to confirm the core "send-and-receive" loop is functional.

**2. The Log is Your Perception:** The session log file (`reports/run_${SESSION_ID}.log`) is your only way to observe the application's state and the outcome of your actions.

**3. The Loop is Your Strategy:** You must operate within a "Test -> Analyze -> [If Failed: Diagnose -> Fix -> Verify]" cycle.

**4. Full Authority, Full Responsibility:** You have the authority to modify any part of the codebase (`.dart` or `.ts` files) to aid in your investigation and to implement a fix. You are responsible for removing any temporary logging before your final commit.

---

### Phase 1: Test Setup & Execution

**Goal:** Launch the application and execute a predefined "happy path" test.

1. **Start Session & Launch Environment:**
    * Execute `SESSION_ID=$(bash reports/run_session.sh)`.
    * This command launches the app and provides the unique `SESSION_ID` for this test run. The log file is `reports/run_${SESSION_ID}.log`.

2. **Wait for App Readiness:**
    * Monitor the log file until the string "A Dart VM Service is available" appears. This confirms the application is running.

3. **Define and Announce the Test Plan:**
    * This rule uses a single, predefined test plan. You must announce it clearly.
    * **Announce:** "My plan is to test the core workflow:
        1. Find and tap the message input field in the Hub.
        2. Type a simple test prompt: 'Hello, world!'.
        3. Find and tap the send button.
        4. Verify that the automation successfully reaches the refinement stage by checking the log for the success string."

4. **Execute the Test Plan:**
    * Use `mobile-mcp` commands to execute the steps you just announced.
    * Example commands: `mobile_mcp_find_and_tap 'hub_message_input'`, `mobile_mcp_type_text 'Hello, world!'`, `mobile_mcp_find_and_tap 'hub_send_button'`.

5. **Wait for Automation to Complete:**
    * Allow a reasonable time (e.g., 30-45 seconds) for the automation to run in the WebView.

---

### Phase 2: Result Analysis & Autonomous Remediation

**Goal:** Determine if the test passed. If not, enter the debugging loop until it does.

1. **Analyze the Log for the Success Signature:**
    * Read the session log file: `reports/run_${SESSION_ID}.log`.
    * Search for the key string that confirms the AI has finished responding and the UI is ready for the user: **`[Observer] Detected finalized response... Notifying Dart that UI is ready for refinement.`**

2. **Evaluate the Outcome:**
    * **If the success string is found:** The test has **PASSED**. Proceed to **Phase 3: Finalization**.
    * **If the success string is NOT found:** The test has **FAILED**. A regression has been detected. You must now begin the **Autonomous Remediation Loop**.

#### **Autonomous Remediation Loop (Execute until test passes)**

1. **Diagnose & Hypothesize:**
    * Thoroughly analyze `reports/run_${SESSION_ID}.log` to understand *why* the test failed. Was there a selector error? A timeout? A Dart exception?
    * Formulate a hypothesis about the root cause.
    * **If evidence is insufficient, add temporary logging** to the relevant `.dart` or `.ts` files to get more insight on the next run.

2. **Implement a Fix:**
    * Based on your hypothesis, implement a code change to fix the issue.
    * If you modify TypeScript, you **MUST** run `npm run build`.

3. **Verify the Fix (Re-run the Test):**
    * **Start a new, clean session:** `SESSION_ID=$(bash reports/run_session.sh)`.
    * **Re-execute the entire test plan** from Phase 1, Step 2.
    * Analyze the new log file for the success signature.
    * If it fails again, analyze the new failure mode. Your change may have altered the problem, providing new clues. Refine your hypothesis and **continue the loop.**

---

### Phase 3: Finalization

**Goal:** Clean up your work and report the final outcome.

1. **Code Cleanup:** If you added any temporary logging, you **MUST** remove it now. The final commit must be clean.

2. **Commit the Solution (If Applicable):** If you implemented a fix, use your `git` tool to commit it. The commit message must follow the project's **Version Control Protocol (VCP)** (e.g., `fix(automation): ...`).

3. **Clean Shutdown:** Terminate the final test session by running `bash reports/run_session.sh` one last time.

4. **Final Report:** Announce that the mission is complete.
    * If the test passed on the first try, state: "**Core Workflow Test: PASSED.** No regressions detected."
    * If the test failed and you fixed it, state: "**Core Workflow Test: PASSED AFTER FIX.** A regression was detected and resolved. Root cause: [your analysis]. Solution: [summary of your fix]."
